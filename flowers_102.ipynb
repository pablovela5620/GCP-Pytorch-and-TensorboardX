{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = 'data/flower_data/images/*'\n",
    "LBL_DIR = 'data/flower_data/imagelabels.mat'\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 10\n",
    "num_classes = 102\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Tensorboard\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flowers102_Dataset(Dataset):\n",
    "    def __init__(self, img_dir, lbl_dir, transform=None):\n",
    "        self.img_glob = sorted(glob(img_dir))\n",
    "        self.labels = loadmat(LBL_DIR)['labels'].squeeze()\n",
    "        self.transform = transform\n",
    "        self.classes = 'hi'\n",
    "    def __len__(self):\n",
    "        return len(self.img_glob)\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img_glob[idx])\n",
    "        # Classes must be zero indexed\n",
    "        label = self.labels[idx]-1\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (image, label)\n",
    "    \n",
    "    \n",
    "data_transform = transforms.Compose([\n",
    "                 transforms.Resize(256),\n",
    "                 transforms.CenterCrop(224),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "flower_dataset = Flowers102_Dataset(IMG_DIR, LBL_DIR, data_transform)\n",
    "train_size = int(0.95*len(flower_dataset))\n",
    "test_size = len(flower_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(flower_dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    # Convert from tensor to PIL image\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    # Unnormalize image\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(train_loader))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, str(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained Network and Freeze Layers except final classifer layers\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "set_parameter_requires_grad(resnet18, True)\n",
    "resnet18.fc = nn.Linear(512, num_classes)\n",
    "resnet18 = resnet18.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet18.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, criterion, optimizer, epoch, train_step, device):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        # Move\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward Pass\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(data_loader)}], Loss: {loss.item():.4f}\")\n",
    "            writer.add_scalar('Train/Training Loss', loss.item(), train_step)\n",
    "        train_step +=1\n",
    "        \n",
    "    return train_step\n",
    "\n",
    "def test(data_loader, model, criterion, test_step, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Accuracy Measurement\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            acc =  (predicted == labels).sum().item() / labels.size(0)\n",
    "            test_step +=1\n",
    "                \n",
    "        total_acc = correct /total\n",
    "        print(f'Test Accuracy of the model on the test images: {total_acc}')\n",
    "        writer.add_scalar('Test/Accuracy', total_acc, test_step)\n",
    "        \n",
    "        return test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test SqueezeNet\n",
    "train_step = 0\n",
    "test_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    train_step = train(train_loader, resnet18, criterion, optimizer, epoch, train_step, device)\n",
    "    test_step = test(test_loader, resnet18, criterion, test_step, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
